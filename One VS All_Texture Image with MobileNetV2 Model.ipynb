# Mount Google Drive để lưu mô hình
from google.colab import drive
drive.mount('/content/drive')

# Thư viện cần thiết
import os
import random
import shutil
import numpy as np
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2  # Sử dụng MobileNetV2
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras import Input

# Đường dẫn tới dữ liệu KTH
original_dataset_dir = '/content/drive/MyDrive/Colab Notebooks/NCKH A.Kien/KTH_Dataset'
base_dir = 'brown_bread_prepared'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# Tạo thư mục nếu chưa có
os.makedirs(train_dir, exist_ok=True)
os.makedirs(validation_dir, exist_ok=True)

# Tạo thư mục cho lớp selected_class và lớp other
train_brown_bread_dir = os.path.join(train_dir, 'brown_bread')
validation_brown_bread_dir = os.path.join(validation_dir, 'brown_bread')
train_other_dir = os.path.join(train_dir, 'other')
validation_other_dir = os.path.join(validation_dir, 'other')

os.makedirs(train_brown_bread_dir, exist_ok=True)
os.makedirs(validation_brown_bread_dir, exist_ok=True)
os.makedirs(train_other_dir, exist_ok=True)
os.makedirs(validation_other_dir, exist_ok=True)

# Bước 2: Chọn ngẫu nhiên 11% dữ liệu từ lớp other và toàn bộ dữ liệu của lớp selected_class
def prepare_data():
    # Copy dữ liệu lớp brown_bread (lớp selected_class)
    brown_bread_images = os.listdir(os.path.join(original_dataset_dir, 'brown_bread'))
    random.shuffle(brown_bread_images)
    split_index = int(len(brown_bread_images) * 0.8)  # 80% cho train, 20% cho validation
    train_brown_bread_images = brown_bread_images[:split_index]
    validation_brown_bread_images = brown_bread_images[split_index:]

    # Di chuyển ảnh vào thư mục train/validation cho lớp selected_class
    for fname in train_brown_bread_images:
        src = os.path.join(original_dataset_dir, 'brown_bread', fname)
        dst = os.path.join(train_brown_bread_dir, fname)
        shutil.copyfile(src, dst)

    for fname in validation_brown_bread_images:
        src = os.path.join(original_dataset_dir, 'brown_bread', fname)
        dst = os.path.join(validation_brown_bread_dir, fname)
        shutil.copyfile(src, dst)

    # Các lớp còn lại là lớp other
    other_classes = ['aluminium_foil', 'corduroy', 'cotton', 'cracker', 'linen',
                     'orange_peel', 'sandpaper', 'sponge', 'styrofoam']

    # Lấy ngẫu nhiên 11% ảnh từ mỗi lớp other
    for class_name in other_classes:
        class_dir = os.path.join(original_dataset_dir, class_name)
        images = os.listdir(class_dir)
        random.shuffle(images)
        num_images = int(len(images) * 0.11)  # Lấy ngẫu nhiên 11% số ảnh
        selected_images = images[:num_images]

        # Phân chia thành train và validation (80% train, 20% validation)
        split_index = int(len(selected_images) * 0.8)
        train_images = selected_images[:split_index]
        validation_images = selected_images[split_index:]

        # Di chuyển ảnh vào thư mục train/validation của lớp other
        for fname in train_images:
            src = os.path.join(class_dir, fname)
            dst = os.path.join(train_other_dir, fname)
            shutil.copyfile(src, dst)

        for fname in validation_images:
            src = os.path.join(class_dir, fname)
            dst = os.path.join(validation_other_dir, fname)
            shutil.copyfile(src, dst)

# Bước 3: Tính trọng số cho các lớp
def compute_class_weights():
    selected_class_count = len(os.listdir(train_brown_bread_dir))
    other_class_count = len(os.listdir(train_other_dir))

    # Gộp các nhãn (labels) cho 2 lớp
    class_labels = np.array([0] * other_class_count + [1] * selected_class_count)

    # Tính class_weight cho từng lớp
    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(class_labels), y=class_labels)

    # Chuyển class_weights thành dict (keras cần format dict)
    class_weight_dict = dict(enumerate(class_weights))
    return class_weight_dict

# Bước 4: Xây dựng mô hình MobileNetV2
def build_model():
    # Định nghĩa đầu vào
    inputs = Input(shape=(128, 128, 3))  # Thay đổi kích thước ảnh lên 128x128

    # Load mô hình MobileNetV2 với include_top=False
    base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=inputs)
    base_model.trainable = False  # Khóa các lớp MobileNetV2 để không huấn luyện lại

    # Sử dụng GlobalAveragePooling2D thay vì Flatten
    x = Flatten()(base_model.output)

    # Thêm các lớp Dense
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.5)(x)

    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)

    # Lớp đầu ra
    outputs = Dense(1, activation='sigmoid')(x)

    # Tạo mô hình
    model = models.Model(inputs, outputs)

    # Compile mô hình với learning rate nhỏ hơn
    model.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Bước 5: Sinh dữ liệu và huấn luyện mô hình
def train_model():
    # Sinh dữ liệu từ thư mục với giảm Data Augmentation
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,  # Giảm phạm vi xoay
        width_shift_range=0.1,  # Giảm phạm vi shift
        height_shift_range=0.1,
        shear_range=0.1,  # Giảm shear
        zoom_range=0.1,  # Giảm zoom
        horizontal_flip=True,
        fill_mode='nearest')

    validation_datagen = ImageDataGenerator(rescale=1./255)

    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(128, 128),  # Tăng kích thước ảnh lên 128x128
        batch_size=32,
        class_mode='binary')

    validation_generator = validation_datagen.flow_from_directory(
        validation_dir,
        target_size=(128, 128),
        batch_size=32,
        class_mode='binary')

    # Tính class weight
    class_weight_dict = compute_class_weights()

    # Tạo mô hình
    model = build_model()

    # Đường dẫn lưu mô hình tốt nhất vào Google Drive
    model_save_path = '/content/drive/MyDrive/Colab Notebooks/NCKH A.Kien/Model1/brown_bread.keras'

    # Callback để lưu lại mô hình tốt nhất
    checkpoint = ModelCheckpoint(model_save_path, monitor='val_accuracy', save_best_only=True, mode='max')

    # Callback EarlyStopping để tránh overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    # Huấn luyện mô hình
    history = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
        class_weight=class_weight_dict,
        callbacks=[checkpoint, early_stopping])

    return history

# Bước 6: Đánh giá mô hình
def evaluate_model():
    test_datagen = ImageDataGenerator(rescale=1./255)
    test_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(128, 128),
        batch_size=32,
        class_mode='binary',
        shuffle=False)

    model_save_path = '/content/drive/MyDrive/Colab Notebooks/NCKH A.Kien/Model1/brown_bread.keras'

    loaded_model = models.load_model(model_save_path)

    predictions = loaded_model.predict(test_generator)
    predicted_classes = (predictions > 0.5).astype(int)
    true_classes = test_generator.classes

    # Tính accuracy
    accuracy = np.mean(predicted_classes == true_classes)
    print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Gọi các hàm để chuẩn bị dữ liệu, huấn luyện và đánh giá mô hình
prepare_data()
train_model()
#evaluate_model()

# Mount Google Drive để lưu mô hình
from google.colab import drive
drive.mount('/content/drive')

# Thư viện cần thiết
import os
import random
import shutil
import numpy as np
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3  # Sử dụng InceptionV3
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras import Input
from tensorflow.keras.optimizers.schedules import ExponentialDecay

# Đường dẫn tới dữ liệu KTH
original_dataset_dir = '/content/drive/MyDrive/Colab Notebooks/NCKH A.Kien/KTH_Dataset'
base_dir = 'styrofoam_prepared'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# Tạo thư mục nếu chưa có
os.makedirs(train_dir, exist_ok=True)
os.makedirs(validation_dir, exist_ok=True)

# Tạo thư mục cho lớp selected_class và lớp other
train_styrofoam_dir = os.path.join(train_dir, 'styrofoam')
validation_styrofoam_dir = os.path.join(validation_dir, 'styrofoam')
train_other_dir = os.path.join(train_dir, 'other')
validation_other_dir = os.path.join(validation_dir, 'other')

os.makedirs(train_styrofoam_dir, exist_ok=True)
os.makedirs(validation_styrofoam_dir, exist_ok=True)
os.makedirs(train_other_dir, exist_ok=True)
os.makedirs(validation_other_dir, exist_ok=True)

# Bước 2: Chọn ngẫu nhiên 11% dữ liệu từ lớp other và toàn bộ dữ liệu của lớp selected_class
def prepare_data():
    # Copy dữ liệu lớp styrofoam (lớp selected_class)
    styrofoam_images = os.listdir(os.path.join(original_dataset_dir, 'styrofoam'))
    random.shuffle(styrofoam_images)
    split_index = int(len(styrofoam_images) * 0.8)  # 80% cho train, 20% cho validation
    train_styrofoam_images = styrofoam_images[:split_index]
    validation_styrofoam_images = styrofoam_images[split_index:]

    # Di chuyển ảnh vào thư mục train/validation cho lớp selected_class
    for fname in train_styrofoam_images:
        src = os.path.join(original_dataset_dir, 'styrofoam', fname)
        dst = os.path.join(train_styrofoam_dir, fname)
        shutil.copyfile(src, dst)

    for fname in validation_styrofoam_images:
        src = os.path.join(original_dataset_dir, 'styrofoam', fname)
        dst = os.path.join(validation_styrofoam_dir, fname)
        shutil.copyfile(src, dst)

    # Các lớp còn lại là lớp other
    other_classes = ['aluminium_foil', 'brown_bread', 'corduroy', 'cotton', 'cracker',
                     'linen', 'orange_peel', 'sandpaper', 'sponge']

    # Lấy ngẫu nhiên 11% ảnh từ mỗi lớp other
    for class_name in other_classes:
        class_dir = os.path.join(original_dataset_dir, class_name)
        images = os.listdir(class_dir)
        random.shuffle(images)
        num_images = int(len(images) * 0.11)  # Lấy ngẫu nhiên 11% số ảnh
        selected_images = images[:num_images]

        # Phân chia thành train và validation (80% train, 20% validation)
        split_index = int(len(selected_images) * 0.8)
        train_images = selected_images[:split_index]
        validation_images = selected_images[split_index:]

        # Di chuyển ảnh vào thư mục train/validation của lớp other
        for fname in train_images:
            src = os.path.join(class_dir, fname)
            dst = os.path.join(train_other_dir, fname)
            shutil.copyfile(src, dst)

        for fname in validation_images:
            src = os.path.join(class_dir, fname)
            dst = os.path.join(validation_other_dir, fname)
            shutil.copyfile(src, dst)

# Bước 3: Tính trọng số cho các lớp
def compute_class_weights():
    selected_class_count = len(os.listdir(train_styrofoam_dir))
    other_class_count = len(os.listdir(train_other_dir))

    # Gộp các nhãn (labels) cho 2 lớp
    class_labels = np.array([0] * other_class_count + [1] * selected_class_count)

    # Tính class_weight cho từng lớp
    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(class_labels), y=class_labels)

    # Chuyển class_weights thành dict (keras cần format dict)
    class_weight_dict = dict(enumerate(class_weights))
    return class_weight_dict

# Bước 4: Xây dựng mô hình InceptionV3 với các cải tiến
def build_model():
    # Định nghĩa đầu vào
    inputs = Input(shape=(128, 128, 3))  # Thay đổi kích thước ảnh lên 128x128

    # Load mô hình InceptionV3 với include_top=False
    base_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=inputs)
    base_model.trainable = False  # Khóa các lớp InceptionV3 để không huấn luyện lại

    # Sử dụng GlobalAveragePooling2D thay vì Flatten
    x = GlobalAveragePooling2D()(base_model.output)

    # Thêm các lớp Dense
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.3)(x)  # Giảm Dropout từ 0.5 xuống 0.3

    x = Dense(512, activation='relu')(x)
    x = Dropout(0.3)(x)  # Giảm Dropout từ 0.5 xuống 0.3

    # Lớp đầu ra
    outputs = Dense(1, activation='sigmoid')(x)

    # Tạo mô hình
    model = models.Model(inputs, outputs)

    # Giảm learning rate và thêm learning rate decay
    initial_learning_rate = 1e-5
    lr_schedule = ExponentialDecay(
        initial_learning_rate,
        decay_steps=100000,
        decay_rate=0.96,
        staircase=True)

    # Compile mô hình với learning rate decay
    optimizer = optimizers.Adam(learning_rate=lr_schedule)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Bước 5: Sinh dữ liệu và huấn luyện mô hình
def train_model():
    # Sinh dữ liệu từ thư mục với Data Augmentation
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=30,  # Tăng phạm vi xoay
        width_shift_range=0.2,  # Tăng phạm vi shift
        height_shift_range=0.2,
        shear_range=0.2,  # Tăng shear
        zoom_range=0.2,  # Tăng zoom
        horizontal_flip=True,
        fill_mode='nearest')

    validation_datagen = ImageDataGenerator(rescale=1./255)

    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(128, 128),  # Tăng kích thước ảnh lên 128x128
        batch_size=16,  # Giảm batch size xuống
        class_mode='binary')

    validation_generator = validation_datagen.flow_from_directory(
        validation_dir,
        target_size=(128, 128),
        batch_size=16,
        class_mode='binary')

    # Tính class weight
    class_weight_dict = compute_class_weights()

    # Tạo mô hình
    model = build_model()

    # Đường dẫn lưu mô hình tốt nhất vào Google Drive
    model_save_path = '/content/drive/MyDrive/Colab Notebooks/NCKH A.Kien/Model/styrofoam_inceptionv3.keras'

    # Callback để lưu lại mô hình tốt nhất
    checkpoint = ModelCheckpoint(model_save_path, monitor='val_accuracy', save_best_only=True, mode='max')

    # Callback EarlyStopping để tránh overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Huấn luyện mô hình
    history = model.fit(
        train_generator,
        epochs=100,  # Tăng số epochs lên
        validation_data=validation_generator,
        class_weight=class_weight_dict,
        callbacks=[checkpoint, early_stopping])

    return history

# Bước 6: Đánh giá mô hình
def evaluate_model():
    test_datagen = ImageDataGenerator(rescale=1./255)
    test_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(128, 128),
        batch_size=16,
        class_mode='binary',
        shuffle=False)

    model_save_path = '/content/drive/MyDrive/Colab Notebooks/NCKH A.Kien/Model/styrofoam_inceptionv3.keras'

    loaded_model = models.load_model(model_save_path)
    results = loaded_model.evaluate(test_generator)
    print(f"Test Loss: {results[0]}, Test Accuracy: {results[1]}")

# Chạy tất cả các bước
prepare_data()  # Chuẩn bị dữ liệu
history = train_model()  # Huấn luyện mô hình
evaluate_model()  # Đánh giá mô hình
